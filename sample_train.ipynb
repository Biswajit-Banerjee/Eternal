{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local imports\n",
    "from src.data import RNADataset\n",
    "from src.model import RNAStructurePredictor\n",
    "from src.train import train_epoch, validate_epoch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNAPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, 3)  # 3 classes: '.', '(', ')'\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # LSTM\n",
    "        output, _ = self.lstm(packed)\n",
    "        # Unpack\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        # Predict\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, device):\n",
    "    # Sort batch by length for packed sequence\n",
    "    batch = sorted(batch, key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    # Get max length in batch\n",
    "    max_len = max(item['length'] for item in batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    sequences = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            item['sequence'],\n",
    "            (0, 0, 0, max_len - item['length'])\n",
    "        ) for item in batch\n",
    "    ])\n",
    "    \n",
    "    structures = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            item['structure'],\n",
    "            (0, max_len - item['length']),\n",
    "            value=-100  # ignore_index for CrossEntropyLoss\n",
    "        ) for item in batch\n",
    "    ])\n",
    "    \n",
    "    lengths = torch.tensor([item['length'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'sequences': sequences.to(device),\n",
    "        'structures': structures.to(device),\n",
    "        'lengths': lengths.to(device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "val_split = 0.1\n",
    "gradient_accumulation_steps = 4 \n",
    "model_save_dir = Path('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model save directory\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = RNADataset.load(\"data/train_dataset.pkl\")\n",
    "val_dataset = RNADataset.load(\"data/val_dataset.pkl\")\n",
    "test_dataset = RNADataset.load(\"data/test_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, device)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNAPredictor().to(device)\n",
    "    \n",
    "    # Use mixed precision criterion\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:20<00:00, 15.37it/s, loss=0.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:20<00:00, 15.30it/s, loss=1.0210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.8915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 15.16it/s, loss=0.9719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.8084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.32it/s, loss=0.9570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 0.7412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.44it/s, loss=0.9550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 0.6876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.55it/s, loss=0.9267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.59it/s, loss=0.9058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 0.5959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.67it/s, loss=0.9266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 0.5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.51it/s, loss=0.8863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 0.5343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.47it/s, loss=0.8269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 0.5072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.44it/s, loss=0.8090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training Loss: 0.4878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.58it/s, loss=0.8181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training Loss: 0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.36it/s, loss=0.7943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training Loss: 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.31it/s, loss=0.7865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training Loss: 0.4359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.52it/s, loss=0.7837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training Loss: 0.4218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.51it/s, loss=0.7627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training Loss: 0.4105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.53it/s, loss=0.7682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training Loss: 0.4156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.63it/s, loss=0.7632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Training Loss: 0.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.44it/s, loss=0.7596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Training Loss: 0.3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.47it/s, loss=0.7554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Training Loss: 0.3834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.60it/s, loss=0.7639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Training Loss: 0.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.55it/s, loss=0.7604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Training Loss: 0.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.54it/s, loss=0.7495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Training Loss: 0.3690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.40it/s, loss=0.7542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Training Loss: 0.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.39it/s, loss=0.7545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Training Loss: 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.46it/s, loss=0.7330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Training Loss: 0.3586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.52it/s, loss=0.7421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Training Loss: 0.3449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.62it/s, loss=0.7339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Training Loss: 0.3502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.40it/s, loss=0.7461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Training Loss: 0.3403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.51it/s, loss=0.7220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Training Loss: 0.3369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.46it/s, loss=0.7255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Training Loss: 0.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.60it/s, loss=0.7261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Training Loss: 0.3290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.63it/s, loss=0.7400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Training Loss: 0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.51it/s, loss=0.7226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training Loss: 0.3253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.55it/s, loss=0.7210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Training Loss: 0.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.50it/s, loss=0.7095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Training Loss: 0.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.48it/s, loss=0.7083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Training Loss: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.49it/s, loss=0.7136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Training Loss: 0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.52it/s, loss=0.7072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Training Loss: 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.56it/s, loss=0.7096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Training Loss: 0.3163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.54it/s, loss=0.7077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Training Loss: 0.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.53it/s, loss=0.7042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Training Loss: 0.3184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.46it/s, loss=0.7027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Training Loss: 0.3145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.48it/s, loss=0.7077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Training Loss: 0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.53it/s, loss=0.7067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Training Loss: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.52it/s, loss=0.7022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Training Loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.57it/s, loss=0.6987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Training Loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.59it/s, loss=0.6960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Training Loss: 0.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:22<00:00, 14.50it/s, loss=0.7107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training Loss: 0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [00:21<00:00, 14.62it/s, loss=0.7067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Training Loss: 0.3003\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Clear cache before each epoch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, \n",
    "        val_loader,\n",
    "        # train_loader, \n",
    "        criterion, \n",
    "        optimizer\n",
    "    )\n",
    "    print(f'Epoch {epoch+1} - Training Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, 'best_model.pt')\n",
    "    \n",
    "    # Optional: clear cache after each epoch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 34/320 [00:01<00:12, 22.66it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m             sequences \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/workspace/git_repos/Eternal/src/data.py:193\u001b[0m, in \u001b[0;36mRNADataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Get encodings\u001b[39;00m\n\u001b[1;32m    192\u001b[0m seq_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_sequence(seq)\n\u001b[0;32m--> 193\u001b[0m struct_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Create base pair matrix (for advanced training)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m bp_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_bp_matrix(struct)\n",
      "File \u001b[0;32m~/workspace/git_repos/Eternal/src/data.py:184\u001b[0m, in \u001b[0;36mRNADataset.encode_structure\u001b[0;34m(self, struct)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_structure\u001b[39m(\u001b[38;5;28mself\u001b[39m, struct):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Convert structure to indices\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     struct_idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstruct_to_idx[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m struct]\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mLongTensor(struct_idx)\n",
      "File \u001b[0;32m~/workspace/git_repos/Eternal/src/data.py:184\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_structure\u001b[39m(\u001b[38;5;28mself\u001b[39m, struct):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Convert structure to indices\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     struct_idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstruct_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m struct]\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mLongTensor(struct_idx)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'A'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "    \n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"Running inference...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        try:\n",
    "            sequences = batch['sequences'].to(device)\n",
    "            structures = batch['structures'].to(device)\n",
    "            lengths = batch['lengths'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(sequences, lengths)\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=-1)  # (batch_size, seq_len)\n",
    "            \n",
    "            # Convert to flat arrays for evaluation, ignoring padding\n",
    "            for pred, target, length in zip(preds, structures, lengths):\n",
    "                # Only consider predictions up to actual sequence length\n",
    "                pred = pred[:length].cpu().numpy()\n",
    "                target = target[:length].cpu().numpy()\n",
    "                \n",
    "                # Ignore padded positions (target == -100)\n",
    "                mask = target != -100\n",
    "                all_preds.append(pred[mask])\n",
    "                all_targets.append(target[mask])\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "# Calculate per-class metrics\n",
    "per_class_metrics = {}\n",
    "labels = ['.', '(', ')']\n",
    "class_precision, class_recall, class_f1, support = precision_recall_fscore_support(\n",
    "    all_targets, \n",
    "    all_preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
